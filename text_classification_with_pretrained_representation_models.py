# -*- coding: utf-8 -*-
"""Text Classification with Pretrained Representation Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tdr-VveDcV9DGmTQGtFd7s5Vv0ND3Ljp
"""

#pip install datasets

"""#### Using a Pretrained model for classification task"""

from datasets import load_dataset

# Load our data
data = load_dataset("rotten_tomatoes")
data

data["train"][0, -1]

from transformers import pipeline

# Path to our HF model
model_path = "cardiffnlp/twitter-roberta-base-sentiment-latest"

# Load model into pipeline
pipe = pipeline(
    model=model_path,
    tokenizer=model_path,
    return_all_scores=True,
    device="cuda:0"
)

import numpy as np
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset

# Run inference
y_pred = []
for output in tqdm(pipe(KeyDataset(data["test"], "text")), total=len(data["test"])):
    negative_score = output[0]["score"]
    positive_score = output[2]["score"]
    assignment = np.argmax([negative_score, positive_score])
    y_pred.append(assignment)

from sklearn.metrics import classification_report

def evaluate_performance(y_true, y_pred):
    """Create and print the classification report"""
    performance = classification_report(
        y_true, y_pred,
        target_names=["Negative Review", "Positive Review"]
    )
    print(performance)

evaluate_performance(data["test"]["label"], y_pred)

"""#### Using an Embedding model instead of Fine tuning

This saves computation and leaves the pretrained model frozen. The model is used for feature extraction with input text converted to embeddings and then used for training a model for classification
"""

from sentence_transformers import SentenceTransformer

# Load model
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

# Convert text to embeddings
train_embeddings = model.encode(data["train"]["text"], show_progress_bar=True)
test_embeddings = model.encode(data["test"]["text"], show_progress_bar=True)

"""**We can also use Cohere or OpenAI embeddings through their APIs**"""

from sklearn.linear_model import LogisticRegression

# Train a logistic regression on our train embeddings
clf = LogisticRegression(random_state=42)
clf.fit(train_embeddings, data["train"]["label"])

# Predict previously unseen instances
y_pred = clf.predict(test_embeddings)
evaluate_performance(data["test"]["label"], y_pred)

"""By using the embedding model from a pretrained model, we have increased F1 score to 85% !!!

#### Classification with no labelled data - ZERO SHOT

Cosine similarity between a negative review and the new data ( i.e. review)
"""

# Create embeddings for our labels
label_embeddings = model.encode(["A negative review",  "A positive review"])

from sklearn.metrics.pairwise import cosine_similarity

# Find the best matching label for each document
sim_matrix = cosine_similarity(test_embeddings, label_embeddings)
y_pred = np.argmax(sim_matrix, axis=1)

evaluate_performance(data["test"]["label"], y_pred)

"""The example here demonstrates the flexibility of embeddings for a variety of tasks. The F1 score is still good at 78%"""